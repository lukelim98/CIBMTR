{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc523a40-8015-4ead-8996-926c75efe114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "505cf9ac-d44f-4583-bcfc-ac445fc67c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Data/train.csv')\n",
    "test_data = pd.read_csv('Data/test.csv')\n",
    "data_dict = pd.read_csv('Data/data_dictionary.csv')\n",
    "sample_submission = pd.read_csv('Data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330eb940-54d8-4f29-a77f-2b0948f2004d",
   "metadata": {},
   "source": [
    "# Handling NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f600c3a-e336-4be6-a3c0-c3417663df08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>dri_score</th>\n",
       "      <th>psych_disturb</th>\n",
       "      <th>cyto_score</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>hla_match_c_high</th>\n",
       "      <th>hla_high_res_8</th>\n",
       "      <th>tbi_status</th>\n",
       "      <th>arrhythmia</th>\n",
       "      <th>hla_low_res_6</th>\n",
       "      <th>...</th>\n",
       "      <th>tce_div_match</th>\n",
       "      <th>donor_related</th>\n",
       "      <th>melphalan_dose</th>\n",
       "      <th>hla_low_res_8</th>\n",
       "      <th>cardiac</th>\n",
       "      <th>hla_match_drb1_high</th>\n",
       "      <th>pulm_moderate</th>\n",
       "      <th>hla_low_res_10</th>\n",
       "      <th>efs</th>\n",
       "      <th>efs_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>N/A - non-malignant indication</td>\n",
       "      <td>No</td>\n",
       "      <td>Poor</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>No TBI</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Permissive mismatched</td>\n",
       "      <td>Unrelated</td>\n",
       "      <td>N/A, Mel not given</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>No</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>TBI +- Other, &gt;cGy</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Permissive mismatched</td>\n",
       "      <td>Related</td>\n",
       "      <td>N/A, Mel not given</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>N/A - non-malignant indication</td>\n",
       "      <td>No</td>\n",
       "      <td>Poor</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No TBI</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Permissive mismatched</td>\n",
       "      <td>Related</td>\n",
       "      <td>N/A, Mel not given</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>High</td>\n",
       "      <td>No</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No TBI</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Permissive mismatched</td>\n",
       "      <td>Unrelated</td>\n",
       "      <td>N/A, Mel not given</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>High</td>\n",
       "      <td>No</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No TBI</td>\n",
       "      <td>No</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Permissive mismatched</td>\n",
       "      <td>Related</td>\n",
       "      <td>MEL</td>\n",
       "      <td>8.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID                       dri_score psych_disturb    cyto_score diabetes  \\\n",
       "0  0.0  N/A - non-malignant indication            No          Poor       No   \n",
       "1  1.0                    Intermediate            No  Intermediate       No   \n",
       "2  2.0  N/A - non-malignant indication            No          Poor       No   \n",
       "3  3.0                            High            No  Intermediate       No   \n",
       "4  4.0                            High            No  Intermediate       No   \n",
       "\n",
       "   hla_match_c_high  hla_high_res_8          tbi_status arrhythmia  \\\n",
       "0               2.0             7.0              No TBI         No   \n",
       "1               2.0             8.0  TBI +- Other, >cGy         No   \n",
       "2               2.0             8.0              No TBI         No   \n",
       "3               2.0             8.0              No TBI         No   \n",
       "4               2.0             8.0              No TBI         No   \n",
       "\n",
       "   hla_low_res_6  ...          tce_div_match donor_related  \\\n",
       "0            6.0  ...  Permissive mismatched     Unrelated   \n",
       "1            6.0  ...  Permissive mismatched       Related   \n",
       "2            6.0  ...  Permissive mismatched       Related   \n",
       "3            6.0  ...  Permissive mismatched     Unrelated   \n",
       "4            6.0  ...  Permissive mismatched       Related   \n",
       "\n",
       "       melphalan_dose hla_low_res_8 cardiac  hla_match_drb1_high  \\\n",
       "0  N/A, Mel not given           8.0      No                  2.0   \n",
       "1  N/A, Mel not given           8.0      No                  2.0   \n",
       "2  N/A, Mel not given           8.0      No                  2.0   \n",
       "3  N/A, Mel not given           8.0      No                  2.0   \n",
       "4                 MEL           8.0      No                  2.0   \n",
       "\n",
       "  pulm_moderate  hla_low_res_10  efs efs_time  \n",
       "0            No            10.0  0.0   42.356  \n",
       "1           Yes            10.0  1.0    4.672  \n",
       "2            No            10.0  0.0   19.793  \n",
       "3            No            10.0  0.0  102.349  \n",
       "4            No            10.0  0.0   16.223  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "\n",
    "def handle_missing_values(data):\n",
    "    # Separating categorical and numeric columns\n",
    "    cat_cols = [col for col in data.columns if data[col].dtype == 'object']\n",
    "    num_cols = [col for col in data.columns if col not in cat_cols]\n",
    "\n",
    "    # Handling missing numerical data with KNNImputer\n",
    "    knn_imputer = KNNImputer(n_neighbors=5)\n",
    "    data[num_cols] = knn_imputer.fit_transform(data[num_cols])\n",
    "\n",
    "    # Imputing categorical data with KNeighborsClassifier\n",
    "    for col in cat_cols:\n",
    "        missing_mask = data[col].isna()\n",
    "        if missing_mask.sum() > 0:\n",
    "            # Separate training and rpediction sets\n",
    "            X_train = data.loc[~missing_mask, num_cols]\n",
    "            y_train = data.loc[~missing_mask, col]\n",
    "            X_missing = data.loc[missing_mask, num_cols]\n",
    "\n",
    "            # Train a KNeighborsClassifier\n",
    "            knn = KNeighborsClassifier(n_neighbors=5)\n",
    "            knn.fit(X_train, y_train)\n",
    "            imputed_values = knn.predict(X_missing)\n",
    "\n",
    "            # Fill missing values\n",
    "            data.loc[missing_mask, col] = imputed_values\n",
    "\n",
    "    return data\n",
    "\n",
    "train_data = handle_missing_values(train_data)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb53efe-dee5-44c4-b774-d0371b13051f",
   "metadata": {},
   "source": [
    "# Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d0f16a3-6d69-4f74-be6c-0efe7b949fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target and drop unneeded columns\n",
    "y = train_data['efs']\n",
    "X = train_data.drop(columns=['efs', 'efs_time', 'ID'], errors='ignore')\n",
    "\n",
    "# Encode categorical variables\n",
    "cat_cols = [col for col in X.columns if X[col].dtype == 'object']\n",
    "X = pd.get_dummies(X, columns=cat_cols, dummy_na=False)\n",
    "\n",
    "# Scale numeric featuers\n",
    "num_cols = [col for col in X.columns if col not in cat_cols]\n",
    "scaler = StandardScaler()\n",
    "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "\n",
    "# Ensure all numerica\n",
    "X = X.astype(np.float32)\n",
    "\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_array = X.values.astype(np.float32)\n",
    "y_array = y.values.astype(np.float32)\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_array, y_array, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# Create Datasets and Dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2329944-db15-40c7-8e10-c23f5aa2e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "\n",
    "# Define a simple feedforward model\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5574658c-4a03-4f42-be1e-d5189f729daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.6057, Val Loss: 0.5917, Val Acc: 0.6875\n",
      "Epoch 2/100, Train Loss: 0.5837, Val Loss: 0.5982, Val Acc: 0.6759\n",
      "Epoch 3/100, Train Loss: 0.5769, Val Loss: 0.6012, Val Acc: 0.6788\n",
      "Epoch 4/100, Train Loss: 0.5696, Val Loss: 0.6024, Val Acc: 0.6771\n",
      "Epoch 5/100, Train Loss: 0.5593, Val Loss: 0.6073, Val Acc: 0.6813\n",
      "Epoch 6/100, Train Loss: 0.5470, Val Loss: 0.6186, Val Acc: 0.6682\n",
      "Epoch 7/100, Train Loss: 0.5327, Val Loss: 0.6261, Val Acc: 0.6644\n",
      "Epoch 8/100, Train Loss: 0.5171, Val Loss: 0.6455, Val Acc: 0.6576\n",
      "Epoch 9/100, Train Loss: 0.5029, Val Loss: 0.6555, Val Acc: 0.6535\n",
      "Epoch 10/100, Train Loss: 0.4844, Val Loss: 0.6769, Val Acc: 0.6531\n",
      "Epoch 11/100, Train Loss: 0.4429, Val Loss: 0.6839, Val Acc: 0.6517\n",
      "Epoch 12/100, Train Loss: 0.4327, Val Loss: 0.6939, Val Acc: 0.6503\n",
      "Epoch 13/100, Train Loss: 0.4269, Val Loss: 0.6989, Val Acc: 0.6483\n",
      "Epoch 14/100, Train Loss: 0.4224, Val Loss: 0.7058, Val Acc: 0.6453\n",
      "Epoch 15/100, Train Loss: 0.4186, Val Loss: 0.7141, Val Acc: 0.6460\n",
      "Epoch 16/100, Train Loss: 0.4151, Val Loss: 0.7177, Val Acc: 0.6450\n",
      "Epoch 17/100, Train Loss: 0.4115, Val Loss: 0.7254, Val Acc: 0.6443\n",
      "Epoch 18/100, Train Loss: 0.4084, Val Loss: 0.7302, Val Acc: 0.6431\n",
      "Epoch 19/100, Train Loss: 0.4052, Val Loss: 0.7373, Val Acc: 0.6446\n",
      "Epoch 20/100, Train Loss: 0.4028, Val Loss: 0.7412, Val Acc: 0.6425\n",
      "Epoch 21/100, Train Loss: 0.3958, Val Loss: 0.7427, Val Acc: 0.6439\n",
      "Epoch 22/100, Train Loss: 0.3952, Val Loss: 0.7439, Val Acc: 0.6451\n",
      "Epoch 23/100, Train Loss: 0.3948, Val Loss: 0.7452, Val Acc: 0.6438\n",
      "Epoch 24/100, Train Loss: 0.3945, Val Loss: 0.7459, Val Acc: 0.6441\n",
      "Epoch 25/100, Train Loss: 0.3941, Val Loss: 0.7468, Val Acc: 0.6446\n",
      "Epoch 26/100, Train Loss: 0.3938, Val Loss: 0.7475, Val Acc: 0.6446\n",
      "Epoch 27/100, Train Loss: 0.3935, Val Loss: 0.7484, Val Acc: 0.6455\n",
      "Epoch 28/100, Train Loss: 0.3932, Val Loss: 0.7490, Val Acc: 0.6451\n",
      "Epoch 29/100, Train Loss: 0.3929, Val Loss: 0.7498, Val Acc: 0.6446\n",
      "Epoch 30/100, Train Loss: 0.3926, Val Loss: 0.7504, Val Acc: 0.6443\n",
      "Epoch 31/100, Train Loss: 0.3918, Val Loss: 0.7505, Val Acc: 0.6444\n",
      "Epoch 32/100, Train Loss: 0.3918, Val Loss: 0.7506, Val Acc: 0.6444\n",
      "Epoch 33/100, Train Loss: 0.3918, Val Loss: 0.7507, Val Acc: 0.6444\n",
      "Epoch 34/100, Train Loss: 0.3917, Val Loss: 0.7507, Val Acc: 0.6443\n",
      "Epoch 35/100, Train Loss: 0.3917, Val Loss: 0.7508, Val Acc: 0.6443\n",
      "Epoch 36/100, Train Loss: 0.3917, Val Loss: 0.7509, Val Acc: 0.6443\n",
      "Epoch 37/100, Train Loss: 0.3916, Val Loss: 0.7509, Val Acc: 0.6443\n",
      "Epoch 38/100, Train Loss: 0.3916, Val Loss: 0.7510, Val Acc: 0.6441\n",
      "Epoch 39/100, Train Loss: 0.3916, Val Loss: 0.7511, Val Acc: 0.6443\n",
      "Epoch 40/100, Train Loss: 0.3915, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 41/100, Train Loss: 0.3915, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 42/100, Train Loss: 0.3915, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 43/100, Train Loss: 0.3915, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 44/100, Train Loss: 0.3915, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 45/100, Train Loss: 0.3915, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 46/100, Train Loss: 0.3915, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 47/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 48/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 49/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 50/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 51/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 52/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 53/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 54/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 55/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 56/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 57/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 58/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 59/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 60/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 61/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 62/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 63/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 64/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 65/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 66/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 67/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 68/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 69/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 70/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 71/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 72/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 73/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 74/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 75/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 76/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 77/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 78/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 79/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 80/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 81/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 82/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 83/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 84/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 85/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 86/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 87/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 88/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 89/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 90/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 91/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 92/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 93/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 94/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 95/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 96/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 97/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 98/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 99/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n",
      "Epoch 100/100, Train Loss: 0.3914, Val Loss: 0.7512, Val Acc: 0.6443\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss, and optimizer\n",
    "model = SimpleNet(input_dim=X_train.shape[1])\n",
    "criterion = nn.BCELoss() # Binary cross-entropy loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning Rate scheduler\n",
    "# StepLR\n",
    "# Decrease the learning rate every 10 epochs by a factor of 0.1\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# ReduceLROnPlateaus\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "\n",
    "# Training loop \n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.view(-1) # Flatten for loss calculation\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs).view(-1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            preds = (outputs >= 0.5).float()\n",
    "            val_correct += (preds == targets).sum().item()\n",
    "            val_total += targets.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Update the learning rate using scheduler\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "993e4a4a-2a8f-4b9a-a51f-ce421313e34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ImprovedNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32,1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.relu(self.bn1(self.fc1(x))))\n",
    "        x = self.dropout(self.relu(self.bn2(self.fc2(x))))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "def training(model, criterion, optimizer, epochs, train_loader, val_loader, device):\n",
    "    \"\"\"\n",
    "    Train and evaluate the model.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model to train.\n",
    "        train_loader = DataLoader for the training dataset.\n",
    "        val_loader = DataLoader for the validation dataset.\n",
    "        criterion: Loss function.\n",
    "        optimizer: Optimizer for training the model.\n",
    "        scheduler: Learning rate scheduler (optional).\n",
    "        epochs: Number of epochs to train.\n",
    "        device: Device to use for training ('cpu' or 'mps')\n",
    "\n",
    "    Returns:\n",
    "        history: Dictionary containing training and validation metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Move model to the specified device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Learning Rate Scheduler\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_accuracy': [],\n",
    "        'val_auc': []\n",
    "    }\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).view(-1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_targets = []\n",
    "        all_outputs = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                outputs = model(inputs).view(-1)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Collect outputs for metrics\n",
    "                preds = (outputs >= 0.5).float()\n",
    "                val_correct += (preds == targets).sum().item()\n",
    "                val_total += targets.size(0)\n",
    "\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "                all_outputs.extend(outputs.cpu().numpy())\n",
    "\n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        epoch_val_accuracy = val_correct / val_total\n",
    "        epoch_val_auc = roc_auc_score(all_targets, all_outputs)\n",
    "\n",
    "        # Log metrics\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_accuracy'].append(epoch_val_accuracy)\n",
    "        history['val_auc'].append(epoch_val_auc)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{epochs}, \"\n",
    "            f\"Train Loss: {epoch_loss:.4f}, \"\n",
    "            f\"Val Loss: {epoch_val_loss:.4f}, \"\n",
    "            f\"Val Acc: {epoch_val_accuracy:.4f}, \"\n",
    "            f\"Val AUC: {epoch_val_auc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Update the learning rate using scheduler\n",
    "        scheduler.step()\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0e6b54a-5d2f-4902-9dab-4f104dd71337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.6173, Val Loss: 0.5941, Val Acc: 0.6833, Val AUC: 0.7474\n",
      "Epoch 2/100, Train Loss: 0.5967, Val Loss: 0.5918, Val Acc: 0.6833, Val AUC: 0.7490\n",
      "Epoch 3/100, Train Loss: 0.5933, Val Loss: 0.5909, Val Acc: 0.6858, Val AUC: 0.7495\n",
      "Epoch 4/100, Train Loss: 0.5891, Val Loss: 0.5935, Val Acc: 0.6797, Val AUC: 0.7467\n",
      "Epoch 5/100, Train Loss: 0.5840, Val Loss: 0.5892, Val Acc: 0.6842, Val AUC: 0.7502\n",
      "Epoch 6/100, Train Loss: 0.5795, Val Loss: 0.5921, Val Acc: 0.6795, Val AUC: 0.7462\n",
      "Epoch 7/100, Train Loss: 0.5763, Val Loss: 0.5954, Val Acc: 0.6821, Val AUC: 0.7449\n",
      "Epoch 8/100, Train Loss: 0.5749, Val Loss: 0.5988, Val Acc: 0.6790, Val AUC: 0.7456\n",
      "Epoch 9/100, Train Loss: 0.5696, Val Loss: 0.5974, Val Acc: 0.6806, Val AUC: 0.7425\n",
      "Epoch 10/100, Train Loss: 0.5640, Val Loss: 0.5953, Val Acc: 0.6844, Val AUC: 0.7439\n",
      "Epoch 11/100, Train Loss: 0.5511, Val Loss: 0.6001, Val Acc: 0.6832, Val AUC: 0.7434\n",
      "Epoch 12/100, Train Loss: 0.5450, Val Loss: 0.6014, Val Acc: 0.6825, Val AUC: 0.7430\n",
      "Epoch 13/100, Train Loss: 0.5455, Val Loss: 0.6019, Val Acc: 0.6814, Val AUC: 0.7421\n",
      "Epoch 14/100, Train Loss: 0.5430, Val Loss: 0.6028, Val Acc: 0.6814, Val AUC: 0.7432\n",
      "Epoch 15/100, Train Loss: 0.5435, Val Loss: 0.6050, Val Acc: 0.6785, Val AUC: 0.7410\n",
      "Epoch 16/100, Train Loss: 0.5381, Val Loss: 0.6052, Val Acc: 0.6821, Val AUC: 0.7418\n",
      "Epoch 17/100, Train Loss: 0.5357, Val Loss: 0.6075, Val Acc: 0.6830, Val AUC: 0.7406\n",
      "Epoch 18/100, Train Loss: 0.5377, Val Loss: 0.6050, Val Acc: 0.6799, Val AUC: 0.7402\n",
      "Epoch 19/100, Train Loss: 0.5373, Val Loss: 0.6088, Val Acc: 0.6802, Val AUC: 0.7397\n",
      "Epoch 20/100, Train Loss: 0.5345, Val Loss: 0.6090, Val Acc: 0.6807, Val AUC: 0.7383\n",
      "Epoch 21/100, Train Loss: 0.5300, Val Loss: 0.6088, Val Acc: 0.6809, Val AUC: 0.7383\n",
      "Epoch 22/100, Train Loss: 0.5321, Val Loss: 0.6113, Val Acc: 0.6819, Val AUC: 0.7388\n",
      "Epoch 23/100, Train Loss: 0.5320, Val Loss: 0.6114, Val Acc: 0.6804, Val AUC: 0.7385\n",
      "Epoch 24/100, Train Loss: 0.5350, Val Loss: 0.6072, Val Acc: 0.6792, Val AUC: 0.7383\n",
      "Epoch 25/100, Train Loss: 0.5300, Val Loss: 0.6108, Val Acc: 0.6814, Val AUC: 0.7372\n",
      "Epoch 26/100, Train Loss: 0.5320, Val Loss: 0.6099, Val Acc: 0.6802, Val AUC: 0.7386\n",
      "Epoch 27/100, Train Loss: 0.5312, Val Loss: 0.6090, Val Acc: 0.6807, Val AUC: 0.7382\n",
      "Epoch 28/100, Train Loss: 0.5316, Val Loss: 0.6084, Val Acc: 0.6806, Val AUC: 0.7385\n",
      "Epoch 29/100, Train Loss: 0.5300, Val Loss: 0.6107, Val Acc: 0.6799, Val AUC: 0.7379\n",
      "Epoch 30/100, Train Loss: 0.5336, Val Loss: 0.6078, Val Acc: 0.6818, Val AUC: 0.7384\n",
      "Epoch 31/100, Train Loss: 0.5271, Val Loss: 0.6085, Val Acc: 0.6818, Val AUC: 0.7382\n",
      "Epoch 32/100, Train Loss: 0.5311, Val Loss: 0.6102, Val Acc: 0.6823, Val AUC: 0.7385\n",
      "Epoch 33/100, Train Loss: 0.5321, Val Loss: 0.6108, Val Acc: 0.6813, Val AUC: 0.7380\n",
      "Epoch 34/100, Train Loss: 0.5296, Val Loss: 0.6087, Val Acc: 0.6811, Val AUC: 0.7387\n",
      "Epoch 35/100, Train Loss: 0.5310, Val Loss: 0.6109, Val Acc: 0.6814, Val AUC: 0.7376\n",
      "Epoch 36/100, Train Loss: 0.5322, Val Loss: 0.6081, Val Acc: 0.6781, Val AUC: 0.7382\n",
      "Epoch 37/100, Train Loss: 0.5324, Val Loss: 0.6106, Val Acc: 0.6806, Val AUC: 0.7387\n",
      "Epoch 38/100, Train Loss: 0.5323, Val Loss: 0.6115, Val Acc: 0.6804, Val AUC: 0.7380\n",
      "Epoch 39/100, Train Loss: 0.5312, Val Loss: 0.6085, Val Acc: 0.6804, Val AUC: 0.7389\n",
      "Epoch 40/100, Train Loss: 0.5308, Val Loss: 0.6069, Val Acc: 0.6802, Val AUC: 0.7387\n",
      "Epoch 41/100, Train Loss: 0.5290, Val Loss: 0.6095, Val Acc: 0.6806, Val AUC: 0.7380\n",
      "Epoch 42/100, Train Loss: 0.5306, Val Loss: 0.6092, Val Acc: 0.6826, Val AUC: 0.7386\n",
      "Epoch 43/100, Train Loss: 0.5291, Val Loss: 0.6091, Val Acc: 0.6781, Val AUC: 0.7390\n",
      "Epoch 44/100, Train Loss: 0.5304, Val Loss: 0.6095, Val Acc: 0.6811, Val AUC: 0.7383\n",
      "Epoch 45/100, Train Loss: 0.5312, Val Loss: 0.6085, Val Acc: 0.6819, Val AUC: 0.7381\n",
      "Epoch 46/100, Train Loss: 0.5312, Val Loss: 0.6082, Val Acc: 0.6792, Val AUC: 0.7386\n",
      "Epoch 47/100, Train Loss: 0.5319, Val Loss: 0.6086, Val Acc: 0.6804, Val AUC: 0.7384\n",
      "Epoch 48/100, Train Loss: 0.5284, Val Loss: 0.6094, Val Acc: 0.6788, Val AUC: 0.7386\n",
      "Epoch 49/100, Train Loss: 0.5295, Val Loss: 0.6079, Val Acc: 0.6797, Val AUC: 0.7386\n",
      "Epoch 50/100, Train Loss: 0.5304, Val Loss: 0.6114, Val Acc: 0.6825, Val AUC: 0.7383\n",
      "Epoch 51/100, Train Loss: 0.5287, Val Loss: 0.6067, Val Acc: 0.6813, Val AUC: 0.7385\n",
      "Epoch 52/100, Train Loss: 0.5334, Val Loss: 0.6102, Val Acc: 0.6797, Val AUC: 0.7375\n",
      "Epoch 53/100, Train Loss: 0.5297, Val Loss: 0.6083, Val Acc: 0.6797, Val AUC: 0.7382\n",
      "Epoch 54/100, Train Loss: 0.5304, Val Loss: 0.6115, Val Acc: 0.6806, Val AUC: 0.7377\n",
      "Epoch 55/100, Train Loss: 0.5311, Val Loss: 0.6075, Val Acc: 0.6807, Val AUC: 0.7386\n",
      "Epoch 56/100, Train Loss: 0.5342, Val Loss: 0.6105, Val Acc: 0.6792, Val AUC: 0.7378\n",
      "Epoch 57/100, Train Loss: 0.5306, Val Loss: 0.6081, Val Acc: 0.6797, Val AUC: 0.7379\n",
      "Epoch 58/100, Train Loss: 0.5299, Val Loss: 0.6091, Val Acc: 0.6807, Val AUC: 0.7380\n",
      "Epoch 59/100, Train Loss: 0.5294, Val Loss: 0.6081, Val Acc: 0.6806, Val AUC: 0.7379\n",
      "Epoch 60/100, Train Loss: 0.5336, Val Loss: 0.6088, Val Acc: 0.6807, Val AUC: 0.7388\n",
      "Epoch 61/100, Train Loss: 0.5295, Val Loss: 0.6093, Val Acc: 0.6786, Val AUC: 0.7382\n",
      "Epoch 62/100, Train Loss: 0.5327, Val Loss: 0.6078, Val Acc: 0.6809, Val AUC: 0.7382\n",
      "Epoch 63/100, Train Loss: 0.5294, Val Loss: 0.6102, Val Acc: 0.6802, Val AUC: 0.7376\n",
      "Epoch 64/100, Train Loss: 0.5320, Val Loss: 0.6112, Val Acc: 0.6804, Val AUC: 0.7378\n",
      "Epoch 65/100, Train Loss: 0.5325, Val Loss: 0.6084, Val Acc: 0.6788, Val AUC: 0.7388\n",
      "Epoch 66/100, Train Loss: 0.5303, Val Loss: 0.6129, Val Acc: 0.6819, Val AUC: 0.7382\n",
      "Epoch 67/100, Train Loss: 0.5282, Val Loss: 0.6066, Val Acc: 0.6823, Val AUC: 0.7385\n",
      "Epoch 68/100, Train Loss: 0.5321, Val Loss: 0.6071, Val Acc: 0.6816, Val AUC: 0.7391\n",
      "Epoch 69/100, Train Loss: 0.5296, Val Loss: 0.6109, Val Acc: 0.6811, Val AUC: 0.7378\n",
      "Epoch 70/100, Train Loss: 0.5328, Val Loss: 0.6127, Val Acc: 0.6800, Val AUC: 0.7379\n",
      "Epoch 71/100, Train Loss: 0.5304, Val Loss: 0.6079, Val Acc: 0.6795, Val AUC: 0.7385\n",
      "Epoch 72/100, Train Loss: 0.5295, Val Loss: 0.6087, Val Acc: 0.6802, Val AUC: 0.7382\n",
      "Epoch 73/100, Train Loss: 0.5310, Val Loss: 0.6090, Val Acc: 0.6793, Val AUC: 0.7383\n",
      "Epoch 74/100, Train Loss: 0.5306, Val Loss: 0.6101, Val Acc: 0.6792, Val AUC: 0.7380\n",
      "Epoch 75/100, Train Loss: 0.5320, Val Loss: 0.6093, Val Acc: 0.6821, Val AUC: 0.7389\n",
      "Epoch 76/100, Train Loss: 0.5296, Val Loss: 0.6110, Val Acc: 0.6792, Val AUC: 0.7382\n",
      "Epoch 77/100, Train Loss: 0.5313, Val Loss: 0.6102, Val Acc: 0.6797, Val AUC: 0.7376\n",
      "Epoch 78/100, Train Loss: 0.5339, Val Loss: 0.6082, Val Acc: 0.6806, Val AUC: 0.7382\n",
      "Epoch 79/100, Train Loss: 0.5303, Val Loss: 0.6092, Val Acc: 0.6818, Val AUC: 0.7384\n",
      "Epoch 80/100, Train Loss: 0.5313, Val Loss: 0.6092, Val Acc: 0.6823, Val AUC: 0.7387\n",
      "Epoch 81/100, Train Loss: 0.5305, Val Loss: 0.6091, Val Acc: 0.6806, Val AUC: 0.7378\n",
      "Epoch 82/100, Train Loss: 0.5286, Val Loss: 0.6114, Val Acc: 0.6806, Val AUC: 0.7380\n",
      "Epoch 83/100, Train Loss: 0.5322, Val Loss: 0.6104, Val Acc: 0.6795, Val AUC: 0.7381\n",
      "Epoch 84/100, Train Loss: 0.5304, Val Loss: 0.6109, Val Acc: 0.6802, Val AUC: 0.7380\n",
      "Epoch 85/100, Train Loss: 0.5277, Val Loss: 0.6098, Val Acc: 0.6816, Val AUC: 0.7378\n",
      "Epoch 86/100, Train Loss: 0.5325, Val Loss: 0.6076, Val Acc: 0.6811, Val AUC: 0.7386\n",
      "Epoch 87/100, Train Loss: 0.5318, Val Loss: 0.6094, Val Acc: 0.6790, Val AUC: 0.7380\n",
      "Epoch 88/100, Train Loss: 0.5311, Val Loss: 0.6100, Val Acc: 0.6799, Val AUC: 0.7380\n",
      "Epoch 89/100, Train Loss: 0.5304, Val Loss: 0.6089, Val Acc: 0.6799, Val AUC: 0.7381\n",
      "Epoch 90/100, Train Loss: 0.5321, Val Loss: 0.6074, Val Acc: 0.6792, Val AUC: 0.7382\n",
      "Epoch 91/100, Train Loss: 0.5306, Val Loss: 0.6080, Val Acc: 0.6804, Val AUC: 0.7385\n",
      "Epoch 92/100, Train Loss: 0.5293, Val Loss: 0.6091, Val Acc: 0.6814, Val AUC: 0.7387\n",
      "Epoch 93/100, Train Loss: 0.5294, Val Loss: 0.6103, Val Acc: 0.6825, Val AUC: 0.7388\n",
      "Epoch 94/100, Train Loss: 0.5333, Val Loss: 0.6087, Val Acc: 0.6795, Val AUC: 0.7379\n",
      "Epoch 95/100, Train Loss: 0.5318, Val Loss: 0.6098, Val Acc: 0.6804, Val AUC: 0.7382\n",
      "Epoch 96/100, Train Loss: 0.5313, Val Loss: 0.6079, Val Acc: 0.6800, Val AUC: 0.7384\n",
      "Epoch 97/100, Train Loss: 0.5318, Val Loss: 0.6094, Val Acc: 0.6814, Val AUC: 0.7385\n",
      "Epoch 98/100, Train Loss: 0.5289, Val Loss: 0.6117, Val Acc: 0.6830, Val AUC: 0.7379\n",
      "Epoch 99/100, Train Loss: 0.5289, Val Loss: 0.6080, Val Acc: 0.6807, Val AUC: 0.7382\n",
      "Epoch 100/100, Train Loss: 0.5317, Val Loss: 0.6078, Val Acc: 0.6821, Val AUC: 0.7388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': [0.6172730491807064,\n",
       "  0.5966577172279358,\n",
       "  0.5933147323628266,\n",
       "  0.5891298026260402,\n",
       "  0.5839842617511749,\n",
       "  0.5794732965942886,\n",
       "  0.5762583831532134,\n",
       "  0.5748873870819807,\n",
       "  0.5696068338635895,\n",
       "  0.5640071781145202,\n",
       "  0.5510744279043542,\n",
       "  0.5449926531149282,\n",
       "  0.5455320627325111,\n",
       "  0.5430314879450533,\n",
       "  0.543470657699638,\n",
       "  0.5381491415202617,\n",
       "  0.5357132205118735,\n",
       "  0.5377162483003405,\n",
       "  0.5372837848961354,\n",
       "  0.5344616406079795,\n",
       "  0.5300472452822659,\n",
       "  0.532050444641047,\n",
       "  0.5320482001536422,\n",
       "  0.5349948260519239,\n",
       "  0.5300473605593046,\n",
       "  0.5319718196988106,\n",
       "  0.5311547040939331,\n",
       "  0.5315814144909382,\n",
       "  0.5299713527990713,\n",
       "  0.5335982034189833,\n",
       "  0.5270945094939735,\n",
       "  0.5311316789024406,\n",
       "  0.5320541260557042,\n",
       "  0.5296011115527816,\n",
       "  0.5309965340627565,\n",
       "  0.5322035948435465,\n",
       "  0.5323977562702364,\n",
       "  0.5322577784458796,\n",
       "  0.5311930317017767,\n",
       "  0.5307804639140765,\n",
       "  0.5289709633423223,\n",
       "  0.530569516080949,\n",
       "  0.5291243636359771,\n",
       "  0.5303948570870691,\n",
       "  0.5311666465467877,\n",
       "  0.5311505870272716,\n",
       "  0.5318646251327461,\n",
       "  0.5283898028648562,\n",
       "  0.5295110769983795,\n",
       "  0.5304012916154331,\n",
       "  0.5286982328527503,\n",
       "  0.5334384764234225,\n",
       "  0.5297480856378873,\n",
       "  0.5303626819203298,\n",
       "  0.5310680340975523,\n",
       "  0.5341882353855504,\n",
       "  0.5305735690726174,\n",
       "  0.5298677722199095,\n",
       "  0.5294150801582469,\n",
       "  0.533640643581748,\n",
       "  0.5295482699655825,\n",
       "  0.5326957960923513,\n",
       "  0.5294339032222827,\n",
       "  0.532017742180162,\n",
       "  0.5324901226494048,\n",
       "  0.5303410929938157,\n",
       "  0.5281695544719696,\n",
       "  0.532084773066971,\n",
       "  0.5296244223912557,\n",
       "  0.5327579795486397,\n",
       "  0.5304116196102566,\n",
       "  0.5295321899569697,\n",
       "  0.5310347555826108,\n",
       "  0.530618619090981,\n",
       "  0.5319695111364127,\n",
       "  0.5295702213628425,\n",
       "  0.5312659619996946,\n",
       "  0.5338766186187665,\n",
       "  0.5302810736414459,\n",
       "  0.5312975126422114,\n",
       "  0.5305296704173088,\n",
       "  0.5285554581218296,\n",
       "  0.5322302137398057,\n",
       "  0.5303751599043608,\n",
       "  0.5276643959598408,\n",
       "  0.5325319835709201,\n",
       "  0.5317668991784255,\n",
       "  0.531053362124496,\n",
       "  0.5304458939780792,\n",
       "  0.5320673708700472,\n",
       "  0.5305801853951481,\n",
       "  0.5293295507215792,\n",
       "  0.5293504620591799,\n",
       "  0.5333299631873767,\n",
       "  0.5318103091170391,\n",
       "  0.531269391377767,\n",
       "  0.531826991132564,\n",
       "  0.5288546159863472,\n",
       "  0.5289440200146701,\n",
       "  0.531748198138343],\n",
       " 'val_loss': [0.5940601035952568,\n",
       "  0.5917664617300034,\n",
       "  0.5908798254198498,\n",
       "  0.5934905898239877,\n",
       "  0.5891969121164746,\n",
       "  0.5921214534176721,\n",
       "  0.5954336868392096,\n",
       "  0.5987994334763951,\n",
       "  0.5973994803097513,\n",
       "  0.5952950899799665,\n",
       "  0.6000620280702909,\n",
       "  0.6014262353380521,\n",
       "  0.60191797034608,\n",
       "  0.6027594874302546,\n",
       "  0.6050280258059502,\n",
       "  0.6052466159065565,\n",
       "  0.6075331128305859,\n",
       "  0.6050160406364335,\n",
       "  0.6088157176971436,\n",
       "  0.6090494030051761,\n",
       "  0.6087924248642391,\n",
       "  0.6113470471567578,\n",
       "  0.6114192388123936,\n",
       "  0.6072431776258681,\n",
       "  0.6108060502343707,\n",
       "  0.609926254219479,\n",
       "  0.6089898505144649,\n",
       "  0.6083568026622136,\n",
       "  0.6107318495710691,\n",
       "  0.6078408401873376,\n",
       "  0.608511761493153,\n",
       "  0.6101588797238138,\n",
       "  0.6107506425844298,\n",
       "  0.6086841467354033,\n",
       "  0.6109466127223439,\n",
       "  0.6081310176187091,\n",
       "  0.6105646676487393,\n",
       "  0.6115355259842343,\n",
       "  0.6084791304336654,\n",
       "  0.6068997224171956,\n",
       "  0.6094877476493518,\n",
       "  0.6091704433163007,\n",
       "  0.6090646487143304,\n",
       "  0.60949131515291,\n",
       "  0.6084592284427749,\n",
       "  0.6081566337082122,\n",
       "  0.6086321994662285,\n",
       "  0.609363252752357,\n",
       "  0.6079158071014616,\n",
       "  0.6114412681923972,\n",
       "  0.6066763020224042,\n",
       "  0.6102089196443558,\n",
       "  0.6083131122920248,\n",
       "  0.6114710238244798,\n",
       "  0.6075464631120364,\n",
       "  0.6104602211051517,\n",
       "  0.608097634712855,\n",
       "  0.6090555363231235,\n",
       "  0.6080522992544704,\n",
       "  0.6088012382388115,\n",
       "  0.6093179745806588,\n",
       "  0.6078178750144111,\n",
       "  0.6102051892214351,\n",
       "  0.6111540880468157,\n",
       "  0.6083807604180442,\n",
       "  0.6129350768195259,\n",
       "  0.6065697196457122,\n",
       "  0.6070596918463707,\n",
       "  0.6109364978141255,\n",
       "  0.6126920855707593,\n",
       "  0.607922756837474,\n",
       "  0.6086676549580362,\n",
       "  0.6089696793092622,\n",
       "  0.6100582753618559,\n",
       "  0.6093178060319688,\n",
       "  0.6109729588031769,\n",
       "  0.610235470202234,\n",
       "  0.6082303315401077,\n",
       "  0.6091650095250871,\n",
       "  0.6092363054553668,\n",
       "  0.6091021503011386,\n",
       "  0.6113913610577584,\n",
       "  0.6104052553574244,\n",
       "  0.6109246025482814,\n",
       "  0.6098266187641356,\n",
       "  0.607574674487114,\n",
       "  0.6093889511293835,\n",
       "  0.6099546843104893,\n",
       "  0.6089365485641691,\n",
       "  0.607427595059077,\n",
       "  0.6079877457684941,\n",
       "  0.6090654563572672,\n",
       "  0.6102963616450627,\n",
       "  0.6087474468681547,\n",
       "  0.6098440099093649,\n",
       "  0.6078685459163454,\n",
       "  0.6094447909129991,\n",
       "  0.6116765808728006,\n",
       "  0.6080184814002779,\n",
       "  0.6077736973762512],\n",
       " 'val_accuracy': [0.6833333333333333,\n",
       "  0.6833333333333333,\n",
       "  0.6857638888888888,\n",
       "  0.6796875,\n",
       "  0.6842013888888889,\n",
       "  0.6795138888888889,\n",
       "  0.6821180555555556,\n",
       "  0.6789930555555556,\n",
       "  0.6805555555555556,\n",
       "  0.684375,\n",
       "  0.6831597222222222,\n",
       "  0.6824652777777778,\n",
       "  0.6814236111111112,\n",
       "  0.6814236111111112,\n",
       "  0.6784722222222223,\n",
       "  0.6821180555555556,\n",
       "  0.6829861111111111,\n",
       "  0.6798611111111111,\n",
       "  0.6802083333333333,\n",
       "  0.6807291666666667,\n",
       "  0.6809027777777777,\n",
       "  0.6819444444444445,\n",
       "  0.6803819444444444,\n",
       "  0.6791666666666667,\n",
       "  0.6814236111111112,\n",
       "  0.6802083333333333,\n",
       "  0.6807291666666667,\n",
       "  0.6805555555555556,\n",
       "  0.6798611111111111,\n",
       "  0.6817708333333333,\n",
       "  0.6817708333333333,\n",
       "  0.6822916666666666,\n",
       "  0.68125,\n",
       "  0.6810763888888889,\n",
       "  0.6814236111111112,\n",
       "  0.678125,\n",
       "  0.6805555555555556,\n",
       "  0.6803819444444444,\n",
       "  0.6803819444444444,\n",
       "  0.6802083333333333,\n",
       "  0.6805555555555556,\n",
       "  0.6826388888888889,\n",
       "  0.678125,\n",
       "  0.6810763888888889,\n",
       "  0.6819444444444445,\n",
       "  0.6791666666666667,\n",
       "  0.6803819444444444,\n",
       "  0.6788194444444444,\n",
       "  0.6796875,\n",
       "  0.6824652777777778,\n",
       "  0.68125,\n",
       "  0.6796875,\n",
       "  0.6796875,\n",
       "  0.6805555555555556,\n",
       "  0.6807291666666667,\n",
       "  0.6791666666666667,\n",
       "  0.6796875,\n",
       "  0.6807291666666667,\n",
       "  0.6805555555555556,\n",
       "  0.6807291666666667,\n",
       "  0.6786458333333333,\n",
       "  0.6809027777777777,\n",
       "  0.6802083333333333,\n",
       "  0.6803819444444444,\n",
       "  0.6788194444444444,\n",
       "  0.6819444444444445,\n",
       "  0.6822916666666666,\n",
       "  0.6815972222222222,\n",
       "  0.6810763888888889,\n",
       "  0.6800347222222223,\n",
       "  0.6795138888888889,\n",
       "  0.6802083333333333,\n",
       "  0.6793402777777777,\n",
       "  0.6791666666666667,\n",
       "  0.6821180555555556,\n",
       "  0.6791666666666667,\n",
       "  0.6796875,\n",
       "  0.6805555555555556,\n",
       "  0.6817708333333333,\n",
       "  0.6822916666666666,\n",
       "  0.6805555555555556,\n",
       "  0.6805555555555556,\n",
       "  0.6795138888888889,\n",
       "  0.6802083333333333,\n",
       "  0.6815972222222222,\n",
       "  0.6810763888888889,\n",
       "  0.6789930555555556,\n",
       "  0.6798611111111111,\n",
       "  0.6798611111111111,\n",
       "  0.6791666666666667,\n",
       "  0.6803819444444444,\n",
       "  0.6814236111111112,\n",
       "  0.6824652777777778,\n",
       "  0.6795138888888889,\n",
       "  0.6803819444444444,\n",
       "  0.6800347222222223,\n",
       "  0.6814236111111112,\n",
       "  0.6829861111111111,\n",
       "  0.6807291666666667,\n",
       "  0.6821180555555556],\n",
       " 'val_auc': [0.7473596378502763,\n",
       "  0.7489781773346087,\n",
       "  0.7494858938627168,\n",
       "  0.74669275646044,\n",
       "  0.7501574387587757,\n",
       "  0.7461720184539181,\n",
       "  0.7449002136854891,\n",
       "  0.7455614019638328,\n",
       "  0.7425435683526473,\n",
       "  0.7439399553587381,\n",
       "  0.7433880869340547,\n",
       "  0.7429680079839227,\n",
       "  0.7421323318948334,\n",
       "  0.743206755276515,\n",
       "  0.7409711794104141,\n",
       "  0.741760281002293,\n",
       "  0.7405795660177449,\n",
       "  0.7402187196531417,\n",
       "  0.7397482506097989,\n",
       "  0.7383460493621837,\n",
       "  0.7382605921247793,\n",
       "  0.7387718819888243,\n",
       "  0.738538524982645,\n",
       "  0.7382774897642095,\n",
       "  0.737212030004878,\n",
       "  0.7386497829168136,\n",
       "  0.7381694417758826,\n",
       "  0.7384820178228306,\n",
       "  0.7378572896840456,\n",
       "  0.7383878392231398,\n",
       "  0.7381629007541677,\n",
       "  0.738545550524487,\n",
       "  0.7379806606213898,\n",
       "  0.738732696423551,\n",
       "  0.7376229030725964,\n",
       "  0.7381706530762001,\n",
       "  0.7386533562527505,\n",
       "  0.7379908961090731,\n",
       "  0.7388992502172164,\n",
       "  0.7386810950300227,\n",
       "  0.7379894425486923,\n",
       "  0.7386198032339539,\n",
       "  0.7389778636078265,\n",
       "  0.7383402351206594,\n",
       "  0.7381286209551806,\n",
       "  0.7386038140697619,\n",
       "  0.7384468295486053,\n",
       "  0.7385626298589647,\n",
       "  0.7386149580326835,\n",
       "  0.7383324827986271,\n",
       "  0.7385247767240407,\n",
       "  0.7375473179327803,\n",
       "  0.7381557540822941,\n",
       "  0.7376737776859343,\n",
       "  0.738641122119543,\n",
       "  0.7377991472688025,\n",
       "  0.7378955667740807,\n",
       "  0.7379958624403753,\n",
       "  0.7378634067506494,\n",
       "  0.7388303272291469,\n",
       "  0.7381650810947392,\n",
       "  0.7382460565209688,\n",
       "  0.7375730580645287,\n",
       "  0.7378344566730595,\n",
       "  0.738789445843429,\n",
       "  0.738179132178423,\n",
       "  0.7385140567162303,\n",
       "  0.7390545389179286,\n",
       "  0.7378353651482977,\n",
       "  0.7379337832991,\n",
       "  0.7385135116310874,\n",
       "  0.7382157134480136,\n",
       "  0.7382922676280838,\n",
       "  0.7380213603120601,\n",
       "  0.7388779313316274,\n",
       "  0.7382097780764575,\n",
       "  0.7375877753633872,\n",
       "  0.7381637486643899,\n",
       "  0.7384092792387608,\n",
       "  0.7386931474681824,\n",
       "  0.7377843088399121,\n",
       "  0.7379866565579617,\n",
       "  0.738073143400636,\n",
       "  0.7379927130595496,\n",
       "  0.7377901836464524,\n",
       "  0.7386443320653846,\n",
       "  0.7379810845765009,\n",
       "  0.7379689715733253,\n",
       "  0.7380753237412077,\n",
       "  0.7381946368224879,\n",
       "  0.7385248372890565,\n",
       "  0.7387388740551706,\n",
       "  0.7388170029256536,\n",
       "  0.7379268183222739,\n",
       "  0.7381636275343583,\n",
       "  0.7383559820247878,\n",
       "  0.7385491844254397,\n",
       "  0.7379452300871009,\n",
       "  0.7381805857388042,\n",
       "  0.7387899909285719]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model, loss, and optimizer\n",
    "model = ImprovedNet(input_dim=X_train.shape[1])\n",
    "criterion = nn.BCELoss() # Binary cross-entropy loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "training(model, criterion, optimizer, 100, train_loader, val_loader, 'mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8487c5-0b94-47ee-b8b8-abc22bdbbdfc",
   "metadata": {},
   "source": [
    "# Non traditional method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c19a1d-3f2c-4d3b-9a33-8539b87bf1b3",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ddd952f-0314-4eb4-b2fe-5410c140d4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.69296\teval-auc:0.66999\n",
      "[10]\ttrain-auc:0.75606\teval-auc:0.72814\n",
      "[20]\ttrain-auc:0.77368\teval-auc:0.73655\n",
      "[30]\ttrain-auc:0.78955\teval-auc:0.74152\n",
      "[40]\ttrain-auc:0.80433\teval-auc:0.74609\n",
      "[50]\ttrain-auc:0.81473\teval-auc:0.74908\n",
      "[60]\ttrain-auc:0.82327\teval-auc:0.75004\n",
      "[69]\ttrain-auc:0.83192\teval-auc:0.75000\n",
      "Validation AUC: 0.7500\n",
      "Validation ACC: 0.6839\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Separate target and features\n",
    "y = train_data['efs']\n",
    "X = train_data.drop(columns=['efs', 'efs_time', 'ID'], errors='ignore')\n",
    "\n",
    "# Encode categorical variables\n",
    "cat_cols = [col for col in X.columns if X[col].dtype=='object']\n",
    "X = pd.get_dummies(X, columns=cat_cols, dummy_na=False)\n",
    "\n",
    "# Scale numeric features\n",
    "num_cols = [col for col in X.columns if X[col].dtype != 'uint8']\n",
    "scaler = StandardScaler()\n",
    "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_array = X.values\n",
    "y_array = y.values\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_array, y_array, test_size = 0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to DMatrix for XGBoost\n",
    "train_dmatrix = xgb.DMatrix(X_train, label=y_train)\n",
    "val_dmatrix = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "# Define XGBoost paramters\n",
    "params = {\n",
    "    \"objective\" : \"binary:logistic\", # Binary classification\n",
    "    \"eval_metric\" : \"auc\",           # Evaluation metric\n",
    "    \"eta\" : 0.1,                     # Learning rate\n",
    "    \"max_depth\" : 6,                 # Max depth of tress\n",
    "    \"subsample\" : 0.8,               # Row sampling\n",
    "    \"colsample_bytree\" : 0.8,        # Feature Sampling\n",
    "    \"lambda\" : 1,                    # L2 regularization\n",
    "    \"alpha\" : 0                      # L1 regularization\n",
    "}\n",
    "\n",
    "# Trian the model\n",
    "evals = [(train_dmatrix, 'train'), (val_dmatrix, 'eval')]\n",
    "num_boost_round = 200\n",
    "early_stopping_rounds = 10\n",
    "\n",
    "xgb_model = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=train_dmatrix,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=early_stopping_rounds,\n",
    "    verbose_eval=10\n",
    ")\n",
    "\n",
    "# Make predictions on validation set\n",
    "val_preds = xgb_model.predict(val_dmatrix)\n",
    "val_preds_binary = (val_preds >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate model\n",
    "auc = roc_auc_score(y_val, val_preds)\n",
    "acc = accuracy_score(y_val, val_preds_binary)\n",
    "\n",
    "print(f\"Validation AUC: {auc:.4f}\")\n",
    "print(f\"Validation ACC: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8452b6-2a0a-4956-9963-b05f9b837a51",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7448c3d-e0b2-451f-817f-13b1951a567c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttrain's auc: 0.742947\teval's auc: 0.726273\n",
      "[20]\ttrain's auc: 0.759245\teval's auc: 0.735371\n",
      "[30]\ttrain's auc: 0.774242\teval's auc: 0.741467\n",
      "[40]\ttrain's auc: 0.785755\teval's auc: 0.746016\n",
      "[50]\ttrain's auc: 0.797085\teval's auc: 0.748616\n",
      "[60]\ttrain's auc: 0.805966\teval's auc: 0.750482\n",
      "[70]\ttrain's auc: 0.813671\teval's auc: 0.752309\n",
      "[80]\ttrain's auc: 0.820852\teval's auc: 0.753117\n",
      "[90]\ttrain's auc: 0.828277\teval's auc: 0.75346\n",
      "Early stopping, best iteration is:\n",
      "[88]\ttrain's auc: 0.826883\teval's auc: 0.753681\n",
      "Validation AUC: 0.7537\n",
      "Validation Accuracy: 0.6891\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Separate target and features\n",
    "y = train_data['efs']\n",
    "X = train_data.drop(columns=['efs', 'efs_time', 'ID'], errors='ignore')\n",
    "\n",
    "# Encode categorical variables\n",
    "cat_cols = [col for col in X.columns if X[col].dtype=='object']\n",
    "X = pd.get_dummies(X, columns=cat_cols, dummy_na=False)\n",
    "\n",
    "# Scale numeric features\n",
    "num_cols = [col for col in X.columns if X[col].dtype!='uint8']\n",
    "scaler = StandardScaler()\n",
    "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_array = X.values\n",
    "y_array = y.values\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_array, y_array, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert categorical column indices \n",
    "categorical_features = [X.columns.get_loc(col) for col in cat_cols if col in X.columns]\n",
    "\n",
    "# LightGBM dataset\n",
    "train_dataset = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
    "val_dataset = lgb.Dataset(X_val, label=y_val, categorical_feature=categorical_features, reference=train_dataset)\n",
    "\n",
    "# Define LightGBM parameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc', # Evaluation metric\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31, # Controls complexity of the tree,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.8, # Randomly select a fraction of features for each iteration\n",
    "    'bagging_fraction': 0.8, # Randomly select a fraction of data for each iteration\n",
    "    'bagging_freq': 5, # Frequency of bagging\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "num_round = 200\n",
    "early_stopping_rounds = 10\n",
    "lgb_model = lgb.train(\n",
    "    params=params,\n",
    "    train_set=train_dataset,\n",
    "    num_boost_round=num_round,\n",
    "    valid_sets=[train_dataset, val_dataset],\n",
    "    valid_names=['train', 'eval'],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=early_stopping_rounds),\n",
    "        lgb.log_evaluation(10)\n",
    "    ]\n",
    ")\n",
    "# Make predictions on validation set\n",
    "val_preds = lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration)\n",
    "val_preds_binary = (val_preds >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate model\n",
    "auc = roc_auc_score(y_val, val_preds)\n",
    "accuracy = accuracy_score(y_val, val_preds_binary)\n",
    "\n",
    "print(f\"Validation AUC: {auc:.4f}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a51b36-e276-48a3-83ad-fa18200ebf91",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3177abf-787a-4e6c-b857-7ed20096ca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.7297\n",
      "Validation Accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "# Separate target and features \n",
    "y = train_data['efs']\n",
    "X = train_data.drop(columns=['efs', 'efs_time', 'ID'], errors='ignore')\n",
    "\n",
    "# Encode categorical variables\n",
    "cat_cols = [col for col in X.columns if X[col].dtype=='object']\n",
    "X = pd.get_dummies(X, columns=cat_cols, dummy_na=False)\n",
    "\n",
    "# Scale numeric ceatures\n",
    "num_cols = [col for col in X.columns if col not in cat_cols]\n",
    "scaler = StandardScaler()\n",
    "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_array = X.values\n",
    "y_array = y.values\n",
    "\n",
    "# Split into train and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_array, y_array, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,         # Number of trees in the forest\n",
    "    max_depth=10,            # Maximum depth of the tree\n",
    "    min_samples_split=5,     # Maximum samples to split a node\n",
    "    min_samples_leaf=2,      # Minimum samples at leaf node\n",
    "    max_features='sqrt',     # Number of features to consider at each split\n",
    "    random_state=42,         # For reproducibility\n",
    "    n_jobs=-1,               # Use all available cores for training\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on validation set\n",
    "val_preds = rf_model.predict_proba(X_val)[:,1] # Probability of positive class\n",
    "val_preds_binary = (val_preds >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "auc = roc_auc_score(y_val, val_preds)\n",
    "accuracy = accuracy_score(y_val, val_preds_binary)\n",
    "\n",
    "print(f\"Validation AUC: {auc:.4f}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce20f805-b543-48e0-9eba-e9155c53cb0b",
   "metadata": {},
   "source": [
    "## Simple Ensemble (RandomForestClassifier, XGBClassifier, LGBMClassifier)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd727c9-5256-48d5-9b4a-139c5e3da073",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dabaebb-377b-4192-a0d8-2299042fa1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joowanlim/Desktop/CIBMTR/env/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [06:15:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.7531\n",
      "Validation Accuracy: 0.6875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Initialize individual models\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200, \n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    ")\n",
    "\n",
    "lgbm_model = LGBMClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine models in voting classifier\n",
    "voting_ensemble = VotingClassifier(\n",
    "    estimators=[('rf', rf_model), ('xgb', xgb_model),('lgbm', lgbm_model)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Train ensemble model\n",
    "voting_ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "val_preds = voting_ensemble.predict_proba(X_val)[:,1]\n",
    "val_preds_binary = (val_preds >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate model\n",
    "auc = roc_auc_score(y_val, val_preds)\n",
    "accuracy = accuracy_score(y_val, val_preds_binary)\n",
    "\n",
    "print(f\"Validation AUC: {auc:.4f}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f9fdea-eba9-4491-9905-59041f6df114",
   "metadata": {},
   "source": [
    "## Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fcd24df-9f7d-4385-b5aa-675eb86e49e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joowanlim/Desktop/CIBMTR/env/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [06:15:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/joowanlim/Desktop/CIBMTR/env/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [06:15:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.7532\n",
      "Validation Accuracy: 0.6875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize base models\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100,\n",
    "                                  max_depth=10,\n",
    "                                  random_state=42,\n",
    "                                  n_jobs=-1)),\n",
    "    ('xgb', XGBClassifier(n_estimators=200,\n",
    "                          max_depth=6,\n",
    "                          learning_rate=0.1,\n",
    "                          random_state=42,\n",
    "                          use_label_encoder=False,\n",
    "                          eval_metric='logloss'\n",
    "                          )),\n",
    "    ('lgbm', LGBMClassifier(n_estimators=200,\n",
    "                            learning_rate=0.1,\n",
    "                            num_leaves=31,\n",
    "                            random_state=42))\n",
    "]\n",
    "\n",
    "# Meta-modeling \n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Create stacking ensemble\n",
    "stacking_ensemble = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5 # Use cross-validation\n",
    ")\n",
    "\n",
    "# Train stacking ensemble\n",
    "stacking_ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "val_preds = stacking_ensemble.predict_proba(X_val)[:,1]\n",
    "val_preds_binary = (val_preds >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate model\n",
    "auc = roc_auc_score(y_val, val_preds)\n",
    "accuracy_score = accuracy_score(y_val, val_preds_binary)\n",
    "\n",
    "print(f\"Validation AUC: {auc:.4f}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIBMTR_venv",
   "language": "python",
   "name": "cibmtr_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
